% This is "sig-alternate.tex" V1.9 April 2009
% This file should be compiled with V2.4 of "sig-alternate.cls" April 2009
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.4 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.4) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V1.9 - April 2009

\documentclass{sig-alternate}
\usepackage{graphicx}
\graphicspath{ {figures/} }
\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{MM 12}{October 29–November 2, 2012, Nara, Japan}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{MoViMash: Online Mobile Video Mashup}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{4}
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Abhishek Goyal\\
       \affaddr{Dept. of Computer Science}\\
       \affaddr{Indian Institute of Technology Ropar}\\
       \email{2016csb1027@iitrpr.ac.in}
% 2nd. author
\alignauthor
Sahil Gupta\\
       \affaddr{Dept. of ECE}\\
       \affaddr{Indian Institute of Technology Ropar}\\
       \email{2016csb1056@iitrpr.ac.in}
\and  % use '\and' if you need 'another row' of author names
% 3rd. author
\alignauthor Abhishek Dawas\\
       \affaddr{Dept. of Computer Science}\\
       \affaddr{Indian Institute of Technology Ropar}\\
       \email{2016csb1027@iitrpr.ac.in}
% 4th. author
\alignauthor Abhishek Singh\\
       \affaddr{Dept. of Computer Science}\\
       \affaddr{Indian Institute of Technology Ropar}\\
       \email{2016csb1028@iitrpr.ac.in}
}

\maketitle
\begin{abstract}
With the proliferation of mobile video cameras, it is becoming easier
for users to capture videos of live performances and socially
share them with friends and public. As an attendee of such live
performances typically has limited mobility, each video camera is
able to capture only from a range of restricted viewing angles and
distance, producing a rather monotonous video clip. At such performances,
however, multiple video clips can be captured by different
users, likely from different angles and distances. These videos
can be combined to produce a more interesting and representative
mashup of the live performances for broadcasting and sharing. The
earlier works select video shots merely based on the quality of currently
available videos. MoviMash will automatically merge multiple videos with smooth transitions. Shot transition and shot length distributions are learned from professionally edited
videos. Further, we introduce view quality assessment in the framework
to filter out shaky, occluded, and tilted videos. To the best
of our knowledge, this is the first attempt to incorporate historybased
diversity measurement, state-based video editing rules, and
view quality in automated video mashup generations. Experimental
results have been provided to demonstrate the effectiveness of
MoViMash framework.
\end{abstract}

% A category with the (minimum) three required fields
\category{I.2.10}{[Vision and Scene
Understanding}{Video Analysis}

\terms{Algorithms, Design.}

\keywords{Mobile Video, Virtual Director, Video Mashup.}

\section{Introduction}
Worldwide shipment of camera phones were estimated to reach 1.14 billion in the year 2011 alone \cite{bloomberg:camera}. Furthermore, a survey of over 2,500 respondents by Photobucket reveals that 45\% of the re-spondents use mobile devices to shoot video at least once weekly during the summer of 2011, validating the significant increase in the amount of mobile video uploaded to Photobucket’s video shar-ing website (14× in Summer 2011 compared to December 2010) \cite{braams:babel}.

Proliferation of such mobile devices with video capture capability has enabled users to capture video of their life events such
as concerts, parades, outdoor performances, etc, and socially share
them with friends and public as it happens. Videos recorded by
a single user at such events are shot from a limited range of angles and distances from the performance stage, as an attendee typically has limited mobility (e.g., constraint by seating arrangement).
The recorded video can be monotonous and uninteresting. Furthermore, videos recorded are typically short (in the order of minutes
or tens of minutes), due to tired arms or power constraint of mobile
devices. There are, however, likely to have more than one users
recording the same performance from different angles at the same
time, especially at a well-attended performance.

These recorded and shared video clips of the same performance
can be cut and joined together to produce a new mashup video,
similar to how a TV director of a live TV show would switch be-
tween different cameras to produce the show. The mashedup video will provide a whole view of the full concert/show observed by combining the individual segments.

In this paper, we introduce MoViMash, our approach to solve
the above video selection problem. MoViMash aims to produce
mashup video from a set of mobile devices that is interesting and
pleasing to watch, and uses a combinations of content-analysis,
state-based transitions, history-based diversity, and learning from
human editors to achieve this goal.

We now provide an overview of how MoViMash works in the
usual setting of live performances, shown in Figure 1. There is
generally a staging area and an audience area where the audiences
either sit or stand to watch the performance, and record the perfor-
mance with a mobile device. This setting poses a few challenges to
video mashup.

Since the videos are recorded with a hand-held mobile device,
from the audience area, and likely by non-professional, there is no
guarantee on the view quality. The videos can be shaky or tilted.
Furthermore, it is common to include the back of the head of other
audiences in the view. As other audiences move, the view can be
temporarily occluded. When MoViMash needs to decide which
video to select, it first filters out the videos with bad views currently
from further consideration for selection. To achieve this, MoVi-
Mash analyzes the video to determine the current shakiness, the tilt
angle, and the level of occlusion in the video. Note that shakiness and tilt angle can be obtained from easily sensory data of mobile
device when available.
\textbf{
\begin{figure}[t]
\includegraphics[width=8cm]{image1.pdf}
\caption{A general performance scenario}
\centering
\end{figure}}

The shooting angle of the remaining videos are then classified as
either center, left, and right; and distance from the stage as near and
far as shown in Figure 1. This classification is done every time we
perform video selection because mobile users may change their po-
sition over time. MoViMash now decides which shooting angle and
distance should be used; and for how long the selected class should
persist. To this end, MoViMash tries to imitate a professional video
editor, by using a finite state machine, whose transition probabili-
ties are learned from analyzing professionally edited videos of the
same type of event. The rationale behind the inclusion of learn-
ing is that, we have observed that there are no generic editing rules
that can be precisely defined to work with all types of events. The
video editors make fine decisions such as shot lengths and transi-
tions based on their experience which is hard to enumerate.

The videos from the selected class are further ranked based on
the video quality and diversity values to make the final selection.
To consider video quality, MoViMash favors video with low blurri-
ness, low blockiness (good compression), good contrast, and good
illumination in each video. To consider diversity, MoViMash stores
a history of recent video selections and favors videos with dissimi-
lar views with recent selections.

We have developed MoViMash’s algorithm such that it is online
and only depends on history information. As such, even though
it is not our main goal in this paper, MoViMash can be applied to
mashup of live video feeds from mobile devices.

We now briefly compare MoViMash to existing work to high-
light the contribution of this paper. There has been few works on
video selection in a lecture broadcast and video streaming [21] \cite{asd:zxc}
and video conferencing \cite{hames:ml}. In these works the camera is mainly
selected based on speaker detection. Live performances are not
speaker centric. In fact, the speech signals are generally noise from
the crowd. In one recent work, Shrestha et al. [15] propose a
method to create a video mashup from a given set of concert record-
ings. In that work, the authors select the shots based on mainly
video quality, mostly ignoring view quality. Also, the diversity is
only calculated based on the comparison of the last image of the
current shot and first image of the next shot. It does not consider
the history of video selection and the time for which a particular
camera is selected. Further, video editing rules, which are subtle in
the case of live performances, are not considered.

\textbf{Contributions.} We now summarize our contributions in this paper as follows:
\begin{itemize}
    \item We propose a state-based approach for shot selection that incorporates the selection history in the decision process. Earlier methods select shots based on only currently available videos.
    \item We include view quality in the framework to filter out the bad views that are occluded, tilted, or shaky. Earlier methods only considered video quality.
    \item We build a comprehensive model to calculate diversity that considers both previously selected videos and shot lengths.
    \item We propose a learning-based approach where the shot transition probabilities and shot lengths are learned from professionally edited videos.
\end{itemize}

\textbf{Organization.} The rest of the paper is organized as follows.
We provide a review of earlier work in Section 2. In Section 3 we
describe proposed mashup framework. We evaluate our system in
Section 4. The conclusions are provided in Section 5.
\begin{figure*}{
\centering
\includegraphics{image2.pdf}}
\end{figure*}
\section{PREVIOUS WORK}
Some very few previous works have been done in this field. In
most of these works, videos are mainly selected to show the speak-
ers. In the work by Machnicki and Rowe [9], an online lecture
webcast system is presented in which the cameras that are focusing
on speaker and the presentation (the screen) are selected iteratively
until anybody from audience asks question. When audience ask
question, the camera that is focusing the person asking question is
selected. The automatic selection of cameras in a lecture webcast
is extended by Zhang et al. [21] to include audio based localization
and speaker tracking. Similar approach is taken by Cutler et al. \cite{asd:zxc}
in a meeting scenario where camera that shows the current speaker
is selected. Ranjan et al. [12] use face tracking and audio analysis to show the close-up of the person talking. Since performers play more important role than speakers in live concerts, a speaker
based selection is not appropriate. Further, the faces are generally
far from the camera which cannot be detected. Therefore, face detection is not a reliable basis to select videos.

Al-Hames et al. \cite{hames:ml} extends the camera selection work to include
the motion features. We do not use motion features in our frame-
work because both performers and audience generate continuous
motion. Also, the movement of the mobile camera can inject erro-
neous motion in the video, which is aesthetically appealing. Yu et
al. [20] propose to customize the camera selection and shot lengths
based on user preferences. At every lecture webcast receiving site,
the user can give score to the videos and specify rules for shot
lengths. While such an interactive selection of cameras is useful
for educational scenarios, people may find it annoying and stress-
ful for performances, particularly when the number of videos is
large.

A camera selection method for sports video broadcast is pro-
posed by Wang et al. [16]. The authors assume one main cam-
era and other sub cameras. The empirical main camera duration is
found to be from 4 to 24 seconds, and sub camera duration is found
to be 1.5 to 8 seconds. They select a sub camera based on the clarity of the view, determined using motion features. In our work, along with shakiness of the videos, we also calculate view quality in terms of occlusion and rotation; and video quality in terms of contrast, blur, illumination, and blockiness. We also include
explicit measurement of diversity in the framework. Engstrom et
al. [8] discuss automatic camera selection for broadcast in a sports
event capture scenario. The work mainly promotes collaborative
video production, i.e., video recorded by production team as well
as the consumers.
In other media production applications, the shots are selected to
convey the story to the audience. For instance, de Lima et al. [7] propose a method to automatically select shots from multiple cameras for storytelling, according to the rules provided by the director.
These methods are not useful for us as live performances generally
do not have any story.

Recently, there has been works on creating video mashups from
given set of videos. In one of the most recent works [15], Shrestha
et al. select the cameras based on video quality. Although the au-
thors refer to term ‘diversity’ in the paper, it is merely a compari-
son of current frame and the next frame of the corresponding cam-
era. The authors completely ignore the selection history and the
time for which each view is selected. The authors also ignore edit-
ing rules corresponding to different views, which we incorporate
through learning based classification and selection. Furthermore,
unlike the method proposed in this paper, the authors rely on the
future video for current shot selection. While this approach is fine
for combining stored videos, it is not suitable for live applications
such as broadcasting and live sharing.

We have provided a comparison of the related work in Table 1.
The works have been compared with respect to the following as-
pects: (1) can the method be applied online (a method that uses fu-
ture information cannot be applied online)? (2) is selection history-
based diversity considered? (3) is learning incorporated? (4) is
video quality (clarity, contrast etc.) considered? (5) is view quality
(view occlusion, tilted view etc.) considered? and (6) what is the
underlying application scenario? It can be easily seen that the pro-
posed method is the first attempt to consider history based diversity
through learning for online video selection for live performances.
\begin{figure}{
\includegraphics{image3.pdf}}
\end{figure}
\section{MOVIMASH FRAMEWORK}
We will show the design features of the MoviMash application first. After an overview of MoViMash, we focus on individual components.
\subsection{Design Principles}
The end goal of the MoViMash is to produce a mashup that users
like. To achieve this goal, we have followed a set of design principles as follows:
\begin{itemize}
    \item \textbf{Video Quality: }In our discussion, video quality includes
sharpness, contrast, illumination, and blockiness (due to video
compression). A good image quality gives pleasing experi-
ence to the viewers [10]. Therefore, in our framework we
give priority to good quality videos.
    \item \textbf{View Quality: }A video that is captured by a tilted camera
(rotated around horizontal axis) may have very good video
quality, yet, users generally do not like tilted views. Similarly, a view in which a person or object is occluding stage area (blocking performance view) may be annoying to the user. Therefore view quality is also important. We measure view quality in terms of occlusion, tilt, and shakiness.
    \item \textbf{Diversity: }While static cameras always record videos from
same perspective, mobile users generally shoot videos from
a number of views and diverse perspectives. We take this
opportunity to include more diversified views in the mashup.
Both temporal and spatial aspects of diversity are considered
in the proposed framework.
    \item \textbf{Learning: }When professionals edit the videos, they make
many decisions based on their experience. Such decisions
include shooting angle, distance from the stage, and shot
length. It is, however, difficult to precisely state this experience in terms of hard-coded rules. Therefore, in this work, we learn the shot transitions and lengths from professionally
edited videos.
\end{itemize}
The above mentioned design principles are met in our framework
through various quality metrics and video selection/filtering phases,
as described in the following section.

\subsection{Framework}
At every time instant, we have a number of videos to choose
from. Once we have chosen the video, we also need to decide when
to switch to another video. Hence, there are two main questions
involved here that need to be answered for combining videos: (1)
which video to select? (2) when to switch to another video? We
use a three-phase method to select the video while the length is
determined based on learned editing rules and overall quality score
of the selected video. Figure 2 shows the block diagram of overall framework. The
proposed framework consists of one offline learning phase and three online selection phases namely filtering, classification, and selection. At any given time, the following steps are taken to select the
most suitable video at current instant:

\begin{itemize}
    \item \textbf{Filtering:} In the filtering step, we determine videos that are
unusable by comparing occlusion, shakiness, and tilt scores
against empirically determined thresholds. The remaining
videos are passed to the classification stage.
    \item \textbf{Classification:} The selected cameras are classified as one
of right, center, and left according to the capturing angle.
Further, according to the viewing distance from the stage,
they are classified as near or far.
    \item \textbf{Class Prediction:} According to the class of currently se-
lected video, and class transition probabilities learned from
professionally edited videos, a most suitable class is pre-
dicted and videos from that class are selected for further con-
sideration.
    \item \textbf{Video Selection:} The classified cameras are further ranked
with respect to a combined score of video quality, diversity,
and shakiness. The video with highest score is selected.
    \item \textbf{Shot Length:} The length of the video is selected based on
learned distributions and video quality. A higher quality video
is generally selected for longer time.
\end{itemize}

While filtering and selection phase ensure view and video quality, the classification and diversity ensure that we select videos recorded with different angles and viewing distances to provide a
complete and interesting coverage of the performance. We now
describe each component of the framework in detail.
\begin{figure*}{
\centering
\includegraphics{image4.pdf}}
\end{figure*}
\subsection{View Quality}
The view quality is measured in terms of three characteristics:
occlusion, shakiness, and camera tilt. The details of measurement
of each of these quantities is given below.

\subsubsection{Occlusion}
For both a stand mounted camera and a mobile camera, there
is always a chance of view occlusion. At crowded places, people
sometime do not notice the cameras recording the video and oc-
clude the performance view. Even if people notice the cameras,
they stand in front of or walk across the cameras, because the main
purpose of the performances is to entertain the audience who are
present at the venue rather than video recording. Therefore, we de-
tect the videos which are recorded by occluded cameras and filter
them out.

Occlusion detection methods are popular in the field of object
tracking [13, 19]. There methods employ various appearance mod-
els to seamlessly track multiple objects. In this case, the occlusion
occurs when an object is hidden behind another. In live perfor-
mances, this could be intentionally done by the performers, i.e.,
one performer coming in front of other. We are more interested in
detecting the audience blocking the view. Therefore, those works
are not applicable here. 

We have developed an edge density based method to detect videos
with occluded views. The method is based on the assumption that
the objects that occlude the performance area will result in lower
edge density than the performance area. Therefore, the non-occluded
area of the image, which is far from the camera, will result in more
dense edge points than the occluded area. To differentiate between
homogeneous regions of the stage area, which could also have less
edge density, and occluded area; we perform connected components on the edge image. Following are the steps of the occlusion detection in a given image I:

\begin{itemize}
    \item Edge Detection: In the first step, we calculate the presence of
an edge at each pixel location. Let Ie be the resulting binary
edge image:
\begin{equation}
    I^{e}(x,y)=\left\{\begin{tabular}{cl}1 & if edge is detected at pixel I(x, y)\\0 & otherwise\end{tabular}\right.
\end{equation}
    \item Edge Density: We convolve the edge image with a square
matrix W with all of its elements unity: The output of the operation gives the density of edges around
each pixel.
\begin{equation}
    I^{d} = I^{e} \odot\ \textit{W}
\end{equation}
    \item Labeling the Patches: The image is now divided into patches
of block size b × b. Each patch is labeled as 1 if the sum of
edge densities is less that a threshold, else it is labeled as 0.
\begin{equation}
    I^{e}(x,y)=\left\{\begin{tabular}{cl}1 & if edge is detected at pixel I(x, y)\\0 & otherwise\end{tabular}\right.
\end{equation}
The 1’s in the patch image shows potentially occluded regions.
    \item Connected Components: There can be homogeneous regions
in the non-occluded area as well. These regions, however, are
generally small. Therefore, connected components operation
is performed to find the size of largest group of connected
patches with label 1, which corresponds to occluded region.
    \item Occlusion Score: To calculate the final occlusion score So,
we first calculate the fractional occluded region f in the con-
nected components output image, i.e.,
\begin{equation}
    f = \frac{\textnormal{No\ of\ 1\ patches}}{\textnormal{Total\ number\ of\ patches}}
\end{equation}
\end{itemize}

We also observed that generally the dynamic range of f is very
small. Therefore, we expand its range with an exponential function
to calculate the final score $S^o$:
\begin{equation}
S^{o} = 1 - e^{-f}
\end{equation}
The resulting occlusion scores for an example video sequence
are shown in the Figure 3. The sequence shows a person walking
across a camera, which is recording an outdoor performance. We
can see that as the person enters the camera view, the occlusion
score starts increasing. We obtained similar results for night videos
also, which are not shown due to space limitation. We found that
for a patch size of 20*15 pixels, videos with occlusion score more
than 0.2 are very bad, so these are filtered in the framework.


\subsubsection{Tilt}
In this work, we define tilt as the rotation of the camera around
horizontal axis. User’s generally do not like the videos recorded
by tilted cameras. Therefore, we detect the tilted camera views
and filter them. Here we use the heuristic that for a horizontally
placed camera, most of the lines in the view are horizontal, while
an inclined view generally has non-horizontal lines. The following
steps are taken to calculation tilt:
\begin{itemize}
    \item Line Detection: We use Hough transform to detect the straight
line in the image. Let $l'_i$ be the length of the $i^{th}$ line and $o'_i$ the angle with respect to the horizontal line.
    \item Angle Restriction: We assume that the maximum tilt a cam-
era can have is less than $ \pm \pi/4 $ and any line with the inclina-
tion above this angle is noise and not considered in calcula-
tion. Let the resulting orientation of $l'_i$ line be $o'_i$.
    \item The final tilt score $S^t$ is calculated as absolute of the mean
weighted orientation and normalized by $\pi/4 $:
\begin{equation}
S^t =  \frac{abs \left( \frac{1}{N^t} \sum_{i=1}^{N^l}o_i\ *\  l_i \right)}{\pi/4}
\end{equation}
where $N^l$ is the total number of lines in the image.
\end{itemize}
An example of tilt calculation is shown in Figure 4; the upper
row shows frames from the video and the figure in lower row shows
occlusion scores. The video clip is recorded by a mobile phone
camera. In between, the mobile user gets engaged in some other
activity, and the mobile phone gets tilted. We can observe in the
frames itself the straight lines getting tilted. It gets reflected in the
tilt score as shown in Figure 4 for frames 200 and 216. The videos
with a tilt score of 0.4 are found unusable and they are filtered.

\subsubsection{Shakiness}
Shakiness is calculated based on the method described in \cite{weda:spie}.
In this method, the pixel values are projected on horizontal and
vertical axes. The horizontal and vertical projections are matched
across the frames for calculating camera motion. A median filtered
is finally applied on the motion vectors to differentiate the shaki-
ness from the smooth camera motion. The final value of shakiness
score, $S^s$, is calculated by summing the absolute difference of orig-
inal motion vector and median filtered motion vector. The score is
normalized by calculating maximum difference empirically. For a
shakiness window of 100 frames, the normalization value is 300;
for any value above 300, $S^s$ is saturated to 1.

\subsection{Learning}
As mentioned earlier in Section 1, it is difficult to precisely enu-
merate all the rules which professional editors follow in selecting
a video and its corresponding shot length. In MoViMash, we pro-
pose to learn the behavior of professional editor statistically for use
in creating mashup. We use professionally edited videos for this
purpose. The rules are learned in terms of shooting angle, shoot-
ing distance, and shot length. Following are the steps taken in the
process of learning:
\begin{itemize}
    \item At first, we divide the video into a sequence of shots and
record shot length.
    \item Each shot is classified as right (R), left (L), or center (C)
based on shooting angle (Figure 1).
    \item Depending on the distance of the recording device from the
stage, the videos are further classified as near (N ) or far (F)
(Figure 1).
    \item Based on both classifications, we define six states (also referred as classes in the paper) in which a video can be at any
time instant, i.e., CN , CF, RN , RF, LN , and LF.
    \item From the sequence of the shots, we calculate the state transi-
tion probabilities for the above described six states.
    \item We now feed the transition probabilities (transition matrix)
along with shot lengths (emission matrix) to an hidden Markov
model (HMM). The HMM generates a sequence of shot states
and their corresponding lengths.
\begin{equation}
\bordermatrix{~ & CN & CF & RN & RF & LN & LF \cr
 CN  &  0 & 0.4 & 0.2 & 0.1 & 0.2 & 0.1 \cr
 CF & 0.6 & 0 & 0.1 & 0.1 & 0.1 & 0.1\cr
 RN & 0.6 & 0.1 & 0 & 0.1 & 0.2 & 0.1\cr
 RF & 0.2 & 0.2 & 0.4 & 0 & 0.1 & 0.1\cr
 LN & 0.4 & 0.2 & 0.2 & 0.1 & 0 & 0.1\cr
 LF & 0.2 & 0.2 & 0.1 & 0.1 & 0.4 & 0\cr
}
\end{equation}

\begin{equation}
\bordermatrix{~ & 1 & 2 & 3 & 4 & 5 & 6 & 7 \cr
 CN  &  1/31 & 2/31 & 4/31 & 7/31 & 7/31 & 6/31 & 4/31 \cr
 CF & 3/12 & 4/12 & 2/12 & 1/12 & 1/12 & 1/12 & 0\cr
 RN & 2/15 & 3/15 & 4/15 & 3/15 & 2/15 & 1/15 & 0\cr
 RF & 3/10 & 4/10 & 2/10 & 1/10 & 0 & 0 & 0\cr
 LN & 2/15 & 3/15 & 4/15 & 3/15 & 2/15 & 1/15 & 0\cr
 LF & 3/10 & 4/10 & 2/10 & 1/10 & 0 & 0 & 0\cr
}
\end{equation}
\end{itemize}

We use affine transformation to classify the video, giving an ac-
curacy of ≈ 77\% on our dataset. However, since learning is one
time job, we performed manual classification of shots during the
learning phase to get accurate statistics. Equation 7 shows the
learned transition matrix while Equation 8 emission matrix. We
have carefully selected five videos (live group dances with length
of videos ranging from 210 to 300 seconds), which are profession-
ally edited and aired on television. We downloaded these videos
from YouTube.

These videos include concerts by professional bands and per-
formance at the Academy Awards ceremony. We observed that
in dance videos, the shot lengths are relatively smaller ( average
around 2.3 seconds) compared to solo singing videos ( average
around 3.5 seconds). This finding implies that the learning dataset
should comply with the type of performance for mashup. We also
observed that the average shot lengths for all five dance videos
ranged between 2.2 seconds to 2.4 seconds, showing little varia-
tions, which shows that a particular type of events have similar
pattern of transitions and shot lengths which can be learned and
applied to create online mashup.

\subsection{Video Quality}
We can have different quality videos because of the limitation of
recording devices, varied camera positioning, lighting conditions,
camera angle, and video recording skills of the person. To produce
aesthetically beautiful video, it is important to consider the quality
of the videos.We are considering the following aspects to obtain
video quality score:
\begin{itemize}
    \item \textbf{Blockiness:} The blocking effect mainly comes due to poor
quality of data compression. To measure blockiness, we take
current image as sample and calculate its compression qual-
ity using the method described in [18]. The method generates
a score that takes a value between 1 and 10 (10 represents the
best quality, 1 the worst). We normalize the score between 0
and 1. Let Sb be the blockiness score.
    \item \textbf{Blur:} The video can be blurred due to many reasons such as
out-of-focus recording, camera movement etc. We are calculating blur based on the method described in \cite{crete:blur}. Let Sbr
be the blur score which varies between 0 to 1 (0 represents
blurred and 1 sharp).
    \item \textbf{Illumination:} There can be videos that are recorded in poor
lighting conditions. The purpose of including this metric in
quality measurement is to avoid selecting dark videos. The
illumination score for the image Sim (with width Nw and
height Nh) is calculated as average gray value, normalized
by 255.
\begin{equation}
S^{im} = \frac{1}{255}\frac{1}{N_w*N_h}\sum_{x=0}^{N_w}\sum_{y=0}^{N_h}\textit{I(x,y)}
\end{equation}
    \item \textbf{Contrast:} It has also been found in the literature that an im-
age with good contrast is appreciated by the viewers [10].
Therefore, contrast is also chosen as one of the metrics. The
contrast score $S^c$ is calculated as standard deviation of the
pixel intensities. 
\begin{equation}
S^{c} = \frac{1}{255}\sqrt{\frac{1}{N_w*N_h}\sum_{x=0}^{N_w}\sum_{y=0}^{N_h} (I(x,y)-\overline{I})^{2}}
\end{equation}
Its value varies from 0 to 1 where 1 is the desired value cor-
responding to high contrast.
    \item \textbf{Burned Pixels:} It has been identified that pixels that are
close to 255 or 0 are generally not informative [15]. If $N^b$
is the number of such pixels, the quality score representing
burnt pixels is calculated as follows: 
\begin{equation}
    S^{bp} = \left\{\begin{tabular}{ll}$1-N^b/(0.25*N^i)$ & if$\ N^b/(0.25*N^i)<1$\\0 & otherwise\end{tabular}\right.
\end{equation}
where $N^i$ is the total number of pixels in the image. In this
case, a value of 1 represents best quality, i.e., no burnt pixels;
while a value of 0 means at least 25\% pixels are burnt.
\end{itemize}

The individual quality scores are multiplied to calculate overall
video quality score $S^q$, i.e.,
\begin{equation}
S^q = S^b \times S^{br} \times S^{im} \times S^c \times S^{bp}
\end{equation}
We have chosen to multiply the individual scores because we
want to give priority to the videos that are good in all aspects.
\subsection{Diversity}
The aspect of diversity is included in the framework by calculat-
ing the similarity of the views of the videos selected in the recent
past. Let H be the history of the cameras that have been selected
so far. The history is stored as set of chronologically order tuples,
i.e.,
\begin{equation}
H = \left\{\left(I_j^h,\Delta_j \right)|1\leq j\leq N^v\right\}
\end{equation}
where Nv is the number videos selected in the recent past. Each
tuple has the following two entries:
\begin{itemize}
    \item $I^h$ - Snapshot from the selected cameras at the time of selec-
tion.
    \item Δ - The time for which the particular camera is selected. It
is normalized between 0 to 1 by dividing each video duration
by the total time over which history is stored.
\end{itemize}
Let V S be the view similarity matrix:
\begin{equation}
\textit{V S} = \left\{v^{ij}|1\leq i\leq n 1 \leq j \leq N^v;\forall i=j,v^{ij}=1 \right\}
\end{equation}
where n is number of cameras, and $v^{ij}$ is the view similarity
measure between current frame from the $i^{th}$ video and $j^{th}$ frame
of the history. The motivation of defining the view similarity V S is
to select video with different views. The overall steps of diversity
calculation are as follows:
\begin{enumerate}
    \item Determine the view similarity matrix V S by comparing cur-
rent frame with the frames stored in the history, i.e.,
\begin{equation}
v^{ij} = Diff(I_i^c,I_j^h)
\end{equation}
where Ici is the current frame of ith camera, Ihj is the jth frame of the history, and Diff can be any function to calculate view similarity. We are using SSIM [17] for this purpose.
    \item For the given content, the user interest decreases with time
over which the user watches same or similar content. Hence,
the diversity score of the ith video, i.e., Sd is calculated for each of the current videos as follows:
\begin{equation}
S^d = \sum_{j=1}^{N^v}v^{ij}*\Delta_j
\end{equation}
    \item Store the viewing time of the previous video and the current
frame of the selected video in H. If we choose a scheme
where each camera is selected only for fixed amount of time,
we may just store the current frame of the selected video.
\end{enumerate}

The diversity scores for two candidate videos (Cam 1, Cam 2)
and final mashup created using MoViMash for a performance (P3
in Table 2) are shown in the Figure 5. Although we are showing
diversity for only two videos for clarity, there were five candidate
videos in total. We can see that whenever a video gets selected,
its diversity generally reduces, e.g., diversity of Cam 1 after Shot 8
and diversity of Cam 2 after Shot 3. At Shot 4, Cam 1 gets selected
despite low diversity because its video quality is much better than
others (Figure 5.a-b) with a stable view. Sometimes the diversity
increases even when the video is currently selected (Shot 2, Cam 1)
due to change in camera view, or when one of previous selections
of the video moves out of history window. The diversity of Cam 2
decreases even though it is not selected. It is because during this
time, its view is similar to Cam 1, resulting in large (near 1) value
of $v^{12}$ (Equation 15). At Shot 9, a third (other than Cam 1 and Cam
2) video gets selected until Cam 1’s diversity increases enough so
that it gets selected again. In summary, the metric Sd is able to
capture and spatial and temporal diversity of videos.

\begin{figure}{
\includegraphics{image5.pdf}}
\end{figure}

\subsection{Final Ranking}
For all the videos of the selected class, we have three metrics:
video quality score, diversity score, and shakiness score. We cal-
culated weighted sum of these values to calculate final score \(S^f\):
\begin{equation}
S^f = \alpha_1 S^q + \alpha_2 S^d + \alpha_3 (1-S^s)
\end{equation}
where $\alpha_1$ , $\alpha_2$ , and $\alpha_3$ are weighting coefficients and $\alpha_1$ + $\alpha_2$ + $\alpha_3$ = 1. In the experiments, we are using α1 = α2 = α3, which gives equal weightage to the quality, diversity, and stability of the videos. Nevertheless, these coefficients should be deter-
mined based on the type of performance. For instance, in a hip-hop
video mashup we can give less weightage to shakiness for better diversity and quality. A shaky video, however, can be annoying if theperformance has smooth movements such as a tango performance.
We therefore need to keep α3 higher in this case. Furthermore, if
the videos are generally bad in the quality, we can give set high
value for α1. The shot from the video with the highest score is
selected at the current time instant.

\subsection{Length Selection}
To determine the switching instant, we are using a method which
combines the learning based prediction to obey the editing rules and
the superiority (in terms of overall quality) of the currently selected
video. As discussed in the learning section (Section 3.4), every
class of the videos follows a length distribution. For example the
center videos are generally selected for longer duration while the
left and right videos for relatively smaller duration.

Suppose the length predicted for the current class of the videos
is Le. To accommodate the quality of the selected video in length
calculation, we define bonus length Lb. The purpose of the bonus
length is to reward the high quality videos by extending their length.
Suppose Sf1 is the final combined score of the best camera and Sf2
of the second best camera. Now the length for the currently selected
video, Ls, is determined as follows:
\begin{equation}
L^s = (1-\varsigma)L^e+\varsigma L^b\upsilon
\end{equation}
where υ is the difference of the scores, i.e., υ = Sf1 −Sf2 and ς is
weighting coefficient. In our experiments, we found the empirical
values of Lb = 25 and ς = 0.1 works well. A higher value of
ς will increase impact of the bonus length Lb on the selected shot
length. In this way, ς can be manipulated to override the prediction
made by learned statistics to select longer or shorter shots of given
quality score Sf .

In general cases, camera switching only takes place after the se-
lected length of time. MoViMash, however, performs continuous
check on occlusion and shakiness every second, and whenever the
value goes above threshold (same as the one used in the filtering
step), video selection is triggered. We added this optimization to
take care of the situations where the view gets occluded after it is
selection. While a lower video quality can still be acceptable, an
occluded or shaky video annoys the viewers.

\section{EVALUATION}
Our major aim is to prove that the video created by MoviMash is better than normal conventional applications used to mix videos. In addition, we also compare our result with human-edited versions of mashups. The dataset consists of video
recordings of four performances. For each performance, we cre-
ate three mashups: (1) using proposed framework (MoViMash) (2)
based on ranking average of shakiness, diversity, and video quality
only (VQ-Only) (3) by human editor with editing experience (H-
Edited). Users are asked to rate the quality of all three versions of
mashup.

\begin{figure*}{
\centering
\includegraphics{image6.pdf}}
\end{figure*}

\subsection{Dataset}
The main application of the proposed framework is to combine
the videos recorded by mobile phone users. Therefore, we went to
three public performances and handed over smart phones to the au-
diences for recording. The audience were given a scenario where
they were recording the video for sharing with their friends who
were not present at the performance. All the performances hap-
pened during the night time. The video clips are converted to a
common resolution and synchronized before generating the mashups.
In this work, we synchronized the videos manually as our main fo-
cus is on video selection. The issue of automatic synchronization is
being researched separately [14]. The details of the performances
and video recordings are given in the Table 2. Figure 6 shows se-
lected frames for each performance.
\subsection{User Study}
In the user study, we ask the users to watch the mashups created
by the three methods and rate them accordingly. A total of seven-
teen users participated in the study, with age range from 20 to 30.
The users were mainly graduate students, males and females. The
three methods used to create the mashups were not disclosed to the
users. Further, the order of the videos produced by the different
methods was randomized for different performances to reduce the
bias the users may have due to particular presentation order.

The user study was conducted online with relevant instructions
explained beforehand. It is told to the users that “the main purpose
of this user study is to evaluate quality of video editing”. Users
were asked to rate three distinct performance (P1, P2 and P3). The
three mashups (generated by MoViMash, VQ-Only and H-Edited)
are juxtaposed on a website with questions below them. The users
were allowed to replay the videos while answering the questions.
The users rate the videos with a rating ranged between 1 to 5, where
1 represent worst and 5 being the best. The list of questions asked
are given in Table 3.

\subsection{Result Analysis}
The average responses of the users, along with their standard
deviations, are plotted in Figure 7 and Figure 8. We observed re-
sponses of the users for Questions A,B,C,D to be similar and with
slight variation for Question E.
\subsubsection{Analysis of Questions A, B, C and D}
We observed that majority of users responded homogeneously
for all four of them. The responses indicated that the ratings were
based on some distinct characteristics of the video. Based on additional comments provided by the users, we infer that most of user ratings were based on the quality of the selected video and coverage
of stage areas.

For P2 and P3, the users preferred the MoViMash created mashup.
For both the performances, user’s found the shot transitions to be
smooth and pleasing. We relate this response of the user’s to our
class-based learning for predicting the shot transitions. The users
also remarked that MoViMash produced videos had less occlusions
in comparison with VQ-Only based mashup.

While users liked MoViMash created mashups of P2 and P3,
they preferred VQ-Only mashup for P1. By analyzing the record-
ings and user comments, we found that P1 differs from P2 and P3 in
a common aspect. While P2 and P3 had a large number of similar
quality videos to select from at each time instant, the videos record-
ings of P1 were skewed with respect to quality. There were 2 to 3
videos which were stable and had very high video quality, whereas
all other videos were relatively bad in quality. Since MoViMash
attempts to maximize diversity along with video quality through
class-based shot selection, sometimes poor quality videos are se-
lected.

On the other hand, P2 and P3 had many similar quality videos
to choose from, which allowed MoViMash to select videos of dif-
ferent classes with smooth transitions. Therefore, in the scenarios
where the quality of videos is skewed with only few good quality
videos, our MoViMash should be tuned to give more priority to
the quality rather than smooth transitions. The further tuning can
be done by selecting video from multiple future classes. However,
with proliferation of mobile camera, we envision that in future the
number of video recording will increase. An increased number of
recordings will ensure that there are sufficient number of reason-
ably good quality videos to ensure diversity of shots and smooth
transitions.

The human-edited version received lowest ratings for all three
performances. After discussing with the human editor, we found
that most of video editing techniques they learn assume availability
of high quality videos. The video artifacts such as small camera
shake, illumination variation etc. are generally negligible in these
high quality videos. Therefore, they are generally not comfortable
with the videos captured by mobile devices. Further, it would be
very difficult for a human editor to evaluate and precisely compare
quality of videos, particularly when the number of videos is large.
The performance of video editors can get even worse when they
have to make video selection in real-time. This finding makes a
strong case for automated mashup creation for live applications of
video sharing and broadcasting.

\subsubsection{Overall Video Quality}
User ratings for overall video quality are shown in the Figure 8.
We can see that on average, MoViMash outperforms other meth-
ods in overall video quality. Users generally liked the quality of
videos created by MoViMash for all three performances. Further,
the user ratings related to MoViMash had the least standard devi-
ation among all three mashups. This result implies that the users
had highly correlated opinion about the superiority of MoViMash.
According to the comments users provided to justify their overall
video quality ratings, they liked following things about MoViMash:
complete coverage (from many angles), smooth shot transitions,
less occlusion, and balanced camera selection. Yet, there might
be instances where the semantics of the video govern the video
selection, e.g., celebrity appearance, funny behavior by crowd, in-
teresting gestures and expressions by audiences, etc. It is hard for
the proposed automatic system to identify these instances; and a
human can be introduced in the selection to improve the mashup
quality in such scenarios.

It is important to note that although many users liked VQ-Only
created videos more than MoViMash, they did not specify any concrete aspect they liked about the video except selection of less
shaky videos. Thus, for the given dataset, even though VQ-Only
method is also able to produce videos with reasonable good quality,
it will fail in many real scenarios as it does not have any provision
for view quality and smooth shot transitions.

\subsection{Discussion}
The MoViMash framework takes automatic mashup creation meth-
ods closer to the human editors. It, however, adds to the processing
cost. The enhanced diversity model requires more memory for stor-
ing the history and more number of comparisons. Similarly, occlu-
sion and tilt also require data processing that would have quadratic
complexity in terms of the number of pixels in the image. There-
fore, individual components of the framework could be enabled or
disabled depending on the quality of the candidate videos. For ex-
ample, if the videos are taken from the stand-mounted cameras,
shakiness calculation can be omitted. Furthermore, to make the
system scalable, we can process downscaled images as the video
resolution is not critical for the current system components.

\section{CONCLUSIONS \& FUTURE WORK}
In this paper, we have proposed and validated an online video
mashup creation framework, MoViMash, for videos recorded by
mobile devices. Based on the experiments and user study, we make
following conclusions:
\begin{itemize}
    \item MoViMash creates better quality mashups in comparison to
human editor, and other methods that are mainly based on
video quality.
    \item Proposed learning framework is able to imitate professional
editor’s experience to have smooth shot transitions in the final mashup. The proposed framework is most effective when there are a number of similar visual quality videos to chose
from.
    \item The proposed framework is able to filter occluded and tilted
views providing better viewing experience to the users.
    \item Human editors are not comfortable in editing videos that are
recorded by mobile devices, particularly when there are large
number of videos with varying quality.
    \item Proposed diversity model is able to incorporate both tempo-
ral and spatial aspects of the selection history.
\end{itemize}
In the future, we want to extensively evaluate individual com-
ponents of the system with respect to end-to-end system delay and
output mashup quality. We'd like to extend support to HD functions availabe in most mobiles nowadays. Further, a human
director can only compare a limited number of videos at a time.
A powerful computer, however, can process and compare a large
number of videos simultaneously. Therefore, in the future we want
to study the impact of the number of videos on the quality of the
final mashup.
\section{Acknowledgment}
I acknowledge the help of wikipedia articles in this research.
This research is conducted under the NExT Search Center, sup-
ported by the Singapore National Research Foundation and the In-
teractive Digital Media R\&D Program Office of Media Develop-
ment Authority under research grant WBS:R-252-300-001-490.
\subsection{Type Changes and {\subsecit Special} Characters}
We have already seen several typeface changes in this sample.  You
can indicate italicized words or phrases in your text with
the command \texttt{{\char'134}textit}; emboldening with the
command \texttt{{\char'134}textbf}
and typewriter-style (for instance, for computer code) with
\texttt{{\char'134}texttt}.  But remember, you do not
have to indicate typestyle changes when such changes are
part of the \textit{structural} elements of your
article; for instance, the heading of this subsection will
be in a sans serif\footnote{A third footnote, here.
Let's make this a rather short one to
see how it looks.} typeface, but that is handled by the
document class file. Take care with the use
of\footnote{A fourth, and last, footnote.}
the curly braces in typeface changes; they mark
the beginning and end of
the text that is to be in the different typeface.

You can use whatever symbols, accented characters, or
non-English characters you need anywhere in your document;
you can find a complete list of what is
available in the \textit{\LaTeX\
User's Guide}.

\subsection{Math Equations}
You may want to display math equations in three distinct styles:
inline, numbered or non-numbered display.  Each of
the three are discussed in the next sections.

\subsubsection{Inline (In-text) Equations}
A formula that appears in the running text is called an
inline or in-text formula.  It is produced by the
\textbf{math} environment, which can be
invoked with the usual \texttt{{\char'134}begin. . .{\char'134}end}
construction or with the short form \texttt{\$. . .\$}. You
can use any of the symbols and structures,
from $\alpha$ to $\omega$, available in
\LaTeX; this section will simply show a
few examples of in-text equations in context. Notice how
this equation: \begin{math}\lim_{n\rightarrow \infty}x=0\end{math},
set here in in-line math style, looks slightly different when
set in display style.  (See next section).

\subsubsection{Display Equations}
A numbered display equation -- one set off by vertical space
from the text and centered horizontally -- is produced
by the \textbf{equation} environment. An unnumbered display
equation is produced by the \textbf{displaymath} environment.

Again, in either environment, you can use any of the symbols
and structures available in \LaTeX; this section will just
give a couple of examples of display equations in context.
First, consider the equation, shown as an inline equation above:
\begin{equation}\lim_{n\rightarrow \infty}x=0\end{equation}
Notice how it is formatted somewhat differently in
the \textbf{displaymath}
environment.  Now, we'll enter an unnumbered equation:
\begin{displaymath}\sum_{i=0}^{\infty} x + 1\end{displaymath}
and follow it with another numbered equation:
\begin{equation}\sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f\end{equation}
just to demonstrate \LaTeX's able handling of numbering.

\subsection{Citations}
Citations to articles,
conference proceedings or
books listed
in the Bibliography section of your
article will occur throughout the text of your article.
You should use BibTeX to automatically produce this bibliography;
you simply need to insert one of several citation commands with
a key of the item cited in the proper location in
the \texttt{.tex} file.
The key is a short reference you invent to uniquely
identify each work; in this sample document, the key is
the first author's surname and a
word from the title.  This identifying key is included
with each item in the \texttt{.bib} file for your article.

The details of the construction of the \texttt{.bib} file
are beyond the scope of this sample document, but more
information can be found in the \textit{Author's Guide},
and exhaustive details in the \textit{\LaTeX\ User's
Guide}.

This article shows only the plainest form
of the citation command, using \texttt{{\char'134}cite}.
This is what is stipulated in the SIGS style specifications.
No other citation format is endorsed or supported.

\subsection{Tables}
Because tables cannot be split across pages, the best
placement for them is typically the top of the page
nearest their initial cite.  To
ensure this proper ``floating'' placement of tables, use the
environment \textbf{table} to enclose the table's contents and
the table caption.  The contents of the table itself must go
in the \textbf{tabular} environment, to
be aligned properly in rows and columns, with the desired
horizontal and vertical rules.  Again, detailed instructions
on \textbf{tabular} material
is found in the \textit{\LaTeX\ User's Guide}.

Immediately following this sentence is the point at which
Table 1 is included in the input file; compare the
placement of the table here with the table in the printed
dvi output of this document.

\begin{table}
\centering
\caption{Frequency of Special Characters}
\begin{tabular}{|c|c|l|} \hline
Non-English or Math&Frequency&Comments\\ \hline
\O & 1 in 1,000& For Swedish names\\ \hline
$\pi$ & 1 in 5& Common in math\\ \hline
\$ & 4 in 5 & Used in business\\ \hline
$\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
\hline\end{tabular}
\end{table}

To set a wider table, which takes up the whole width of
the page's live area, use the environment
\textbf{table*} to enclose the table's contents and
the table caption.  As with a single-column table, this wide
table will ``float" to a location deemed more desirable.
Immediately following this sentence is the point at which
Table 2 is included in the input file; again, it is
instructive to compare the placement of the
table here with the table in the printed dvi
output of this document.


\begin{table*}
\centering
\caption{Some Typical Commands}
\begin{tabular}{|c|c|l|} \hline
Command&A Number&Comments\\ \hline
\texttt{{\char'134}alignauthor} & 100& Author alignment\\ \hline
\texttt{{\char'134}numberofauthors}& 200& Author enumeration\\ \hline
\texttt{{\char'134}table}& 300 & For tables\\ \hline
\texttt{{\char'134}table*}& 400& For wider tables\\ \hline\end{tabular}
\end{table*}
% end the environment with {table*}, NOTE not {table}!

\subsection{Figures}
Like tables, figures cannot be split across pages; the
best placement for them
is typically the top or the bottom of the page nearest
their initial cite.  To ensure this proper ``floating'' placement
of figures, use the environment
\textbf{figure} to enclose the figure and its caption.

This sample document contains examples of \textbf{.eps}
and \textbf{.ps} files to be displayable with \LaTeX.  More
details on each of these is found in the \textit{Author's Guide}.

\subsection{Theorem-like Constructs}
Other common constructs that may occur in your article are
the forms for logical constructs like theorems, axioms,
corollaries and proofs.  There are
two forms, one produced by the
command \texttt{{\char'134}newtheorem} and the
other by the command \texttt{{\char'134}newdef}; perhaps
the clearest and easiest way to distinguish them is
to compare the two in the output of this sample document:

This uses the \textbf{theorem} environment, created by
the\linebreak\texttt{{\char'134}newtheorem} command:
\newtheorem{theorem}{Theorem}
\begin{theorem}
Let $f$ be continuous on $[a,b]$.  If $G$ is
an antiderivative for $f$ on $[a,b]$, then
\begin{displaymath}\int^b_af(t)dt = G(b) - G(a).\end{displaymath}
\end{theorem}

The other uses the \textbf{definition} environment, created
by the \texttt{{\char'134}newdef} command:
\newdef{definition}{Definition}
\begin{definition}
If $z$ is irrational, then by $e^z$ we mean the
unique number which has
logarithm $z$: \begin{displaymath}{\log e^z = z}\end{displaymath}
\end{definition}

Two lists of constructs that use one of these
forms is given in the
\textit{Author's  Guidelines}.
 
There is one other similar construct environment, which is
already set up
for you; i.e. you must \textit{not} use
a \texttt{{\char'134}newdef} command to
create it: the \textbf{proof} environment.  Here
is a example of its use:
\begin{proof}
Suppose on the contrary there exists a real number $L$ such that
\begin{displaymath}
\lim_{x\rightarrow\infty} \frac{f(x)}{g(x)} = L.
\end{displaymath}
Then
\begin{displaymath}
l=\lim_{x\rightarrow c} f(x)
= \lim_{x\rightarrow c}
\left[ g{x} \cdot \frac{f(x)}{g(x)} \right ]
= \lim_{x\rightarrow c} g(x) \cdot \lim_{x\rightarrow c}
\frac{f(x)}{g(x)} = 0\cdot L = 0,
\end{displaymath}
which contradicts our assumption that $l\neq 0$.
\end{proof}

Complete rules about using these environments and using the
two different creation commands are in the
\textit{Author's Guide}; please consult it for more
detailed instructions.  If you need to use another construct,
not listed therein, which you want to have the same
formatting as the Theorem
or the Definitionshown above,
use the \texttt{{\char'134}newtheorem} or the
\texttt{{\char'134}newdef} command,
respectively, to create it.

\subsection*{A {\secit Caveat} for the \TeX\ Expert}
Because you have just been given permission to
use the \texttt{{\char'134}newdef} command to create a
new form, you might think you can
use \TeX's \texttt{{\char'134}def} to create a
new command: \textit{Please refrain from doing this!}
Remember that your \LaTeX\ source code is primarily intended
to create camera-ready copy, but may be converted
to other forms -- e.g. HTML. If you inadvertently omit
some or all of the \texttt{{\char'134}def}s recompilation will
be, to say the least, problematic.

\section{Conclusions}
This paragraph will end the body of this sample document.
Remember that you might still have Acknowledgments or
Appendices; brief samples of these
follow.  There is still the Bibliography to deal with; and
we will make a disclaimer about that here: with the exception
of the reference to the \LaTeX\ book, the citations in
this paper are to articles which have nothing to
do with the present subject and are used as
examples only.
%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional


%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns

%\balancecolumns % GM June 2007
% That's all folks!
\end{document}